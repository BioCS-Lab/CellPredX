import torch
import numpy as np

from torch import nn
import torch.nn.functional as F
from torch.autograd import Variable

epsilon = 1e-5


class MultiSimilarityLoss(nn.Module):
    def __init__(self, args):
        super(MultiSimilarityLoss, self).__init__()
        self.thresh = 0.5
        self.margin = 0.1

        self.scale_pos = args.LOSSES.MULTI_SIMILARITY_LOSS.SCALE_POS
        self.scale_neg = args.LOSSES.MULTI_SIMILARITY_LOSS.SCALE_NEG

    def forward(self, feats, labels):
        assert feats.size(0) == labels.size(0), \
            f"feats.size(0): {feats.size(0)} is not equal to labels.size(0): {labels.size(0)}"
        batch_size = feats.size(0)
        sim_mat = torch.matmul(feats, torch.t(feats))

        epsilon = 1e-5
        loss = list()

        for i in range(batch_size):
            pos_pair_ = sim_mat[i][labels == labels[i]]
            pos_pair_ = pos_pair_[pos_pair_ < 1 - epsilon]
            neg_pair_ = sim_mat[i][labels != labels[i]]

            neg_pair = neg_pair_[neg_pair_ + self.margin > min(pos_pair_)]
            pos_pair = pos_pair_[pos_pair_ - self.margin < max(neg_pair_)]

            if len(neg_pair) < 1 or len(pos_pair) < 1:
                continue

            # weighting step
            pos_loss = 1.0 / self.scale_pos * torch.log(
                1 + torch.sum(torch.exp(-self.scale_pos * (pos_pair - self.thresh))))
            neg_loss = 1.0 / self.scale_neg * torch.log(
                1 + torch.sum(torch.exp(self.scale_neg * (neg_pair - self.thresh))))
            loss.append(pos_loss + neg_loss)

        if len(loss) == 0:
            return torch.zeros([], requires_grad=True)

        loss = sum(loss) / batch_size
        return loss


class MultiSimilarityLoss_GA(nn.Module):
    def __init__(self, args):
        super(MultiSimilarityLoss_GA, self).__init__()
        self.thresh = 0.5
        self.margin = 0.1
        self.args=args
        self.scale_pos = args.MULTI_SIMILARITY_LOSS_SCALE_POS
        self.scale_neg = args.MULTI_SIMILARITY_LOSS_SCALE_NEG

        self.pos_mask = self.mask_correlate(args)
        self.neg_mask = torch.logical_not(pos_mask)

    def mask_correlate(self, args):
 
        block = torch.ones((args['NN_COUNT'], args['NN_COUNT']), dtype=torch.long)
        block = block.unsqueeze(0)
        block = torch.repeat_interleave(block, args['batch_size'], dim=0)

        mask = torch.block_diag(*block)
        mask = mask.bool()

        return mask

    def forward(self, feats1, feats2):
        """Override forward functions, utilizing for loops to compute loss sample by sample.
           Under unsupervised scenario the label is generated by memroyBnak using pseudo-labels.
           
        Args:
            feats (_type_): Feature vectors from porjection head of the network
            labels (_type_): Lables of samples. 

        Returns:
            _type_: Loss
        """
        batch_size = feats1.size(0)
        sim_mat = torch.matmul(feats1, torch.t(feats2))
        epsilon = 1e-5

        pos_mask1 = self.pos_mask.logical_and(sim_mat < (1-epsilon))
        sim_pos1 = torch.where(pos_mask1, sim_mat, torch.ones_like(sim_mat).cuda() * float('inf'))
        sim_neg  = torch.where(self.neg_mask, sim_mat, torch.ones_like(sim_mat).cuda() * float('-inf'))

        pos_min, _ = torch.min(sim_pos1, dim=1, keepdim=True)
        neg_max, _ = torch.max(sim_neg,  dim=1, keepdim=True)

        pos_mask2 = pos_mask1.logical_and((sim_mat - self.margin) < neg_max)
        neg_mask1 = self.neg_mask.logical_and((sim_mat + self.margin) > pos_min)

        # sim_pos2 = torch.where(pos_mask2, sim_mat, float('inf'))
        # sim_neg1 = torch.where(neg_mask1, sim_mat, float('-inf'))

        pos_loss = 1./ self.scale_pos * torch.log(
            1. + torch.sum(torch.exp(-self.scale_pos * (sim_mat - self.thresh)) * pos_mask2, dim=1))
        neg_loss = 1./ self.scale_neg * torch.log(
            1. + torch.sum(torch.exp(self.scale_neg * (sim_mat - self.thresh)) * neg_mask1, dim=1))  # ensuring feat is L2-normed

        null_loss_mask = (pos_mask2.sum(dim=1) >= 1).logical_and(neg_mask1.sum(dim=1) >= 1)
        n_null = null_loss_mask.sum()
        
        if n_null < 1:
            return torch.zeros([], requires_grad=True) 

        loss = (pos_loss + neg_loss).sum() / batch_size
        return loss


class InfoNCE(torch.nn.Module):
    def __init__(self, batch_size, temperature=0.5):
        super().__init__()
        self.batch_size = batch_size
        self.register_buffer("temperature", torch.tensor(temperature))
        # self.temperature = temperature
        self.register_buffer("negatives_mask", (~torch.eye(batch_size * 2, batch_size * 2, 
                                                           dtype=bool)).float())
            
    def forward(self, feat1, feat2):
        """
        emb_i and emb_j are batches of embeddings, where corresponding indices are pairs
        z_i, z_j as per SimCLR paper
        """
        #z_i = F.normalize(emb_i, dim=1)
        #z_j = F.normalize(emb_j, dim=1)
        
        # z_i = F.normalize(emb_i, dim=1,p=2)
        # z_j = F.normalize(emb_j, dim=1,p=2)

        features = torch.cat([feat1, feat2], dim=0)
        representations = F.normalize(features, dim=1, p=2)

        similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)
        
        sim_ij = torch.diag(similarity_matrix, self.batch_size)
        sim_ji = torch.diag(similarity_matrix, -self.batch_size)
        positives = torch.cat([sim_ij, sim_ji], dim=0)
        
        nominator = torch.exp(positives / self.temperature)

        # negatives_mask = ~torch.eye(batch_size * 2, batch_size * 2, dtype=bool).float().cuda()
        denominator = self.negatives_mask * torch.exp(similarity_matrix / self.temperature)
    
        loss_partial = -torch.log(nominator / torch.sum(denominator, dim=1))
        loss = torch.sum(loss_partial) / (2 * self.batch_size)
        return loss

class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input, dim=1)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()


class L1regularization(nn.Module):
    def __init__(self, weight_decay=0.1):
        super(L1regularization, self).__init__()
        self.weight_decay = weight_decay

    def forward(self, model):
        regularization_loss = 0.
        for param in model.parameters():
            regularization_loss += torch.mean(abs(param)) * self.weight_decay

        return regularization_loss


class CrossEntropyLabelSmooth(nn.Module):
    """Cross entropy loss with label smoothing regularizer.
    Reference:
    Szegedy et al. Rethinking the Inception Architecture for Computer Vision. CVPR 2016.
    Equation: y = (1 - epsilon) * y + epsilon / K.
    Args:
        num_classes (int): number of classes.
        epsilon (float): weight.
    """

    def __init__(self, num_classes, epsilon=0.1, reduction=True):
        super(CrossEntropyLabelSmooth, self).__init__()
        self.num_classes = num_classes
        self.epsilon = epsilon
        self.reduction = reduction
        self.logsoftmax = nn.LogSoftmax(dim=1)

    def forward(self, inputs, targets):
        """
        Args:
            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)
            targets: ground truth labels with shape (num_classes)
        """
        log_probs = self.logsoftmax(inputs)
        targets = torch.zeros(log_probs.size()).cuda().scatter_(1, targets.unsqueeze(1), 1)
        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes
        loss = (- targets * log_probs).sum(dim=1)
        if self.reduction:
            return loss.mean()
        else:
            return loss
        return loss

import math
def one_hot(label, num_classes):
    onehot = torch.zeros(label.size(0), num_classes).cuda().scatter_(1, label.unsqueeze(1), 1)
    return onehot

class CosineHead(nn.Module):
    def __init__(self, in_features, out_features, scale_factor=30.0):
        super(CosineHead, self).__init__()
        self.scale_factor = scale_factor
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features).float())
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature):
        cosine = F.linear(F.normalize(feature), F.normalize(self.weight))
        return cosine * self.scale_factor


class CosineMarginHead(nn.Module):
    """Implement of large margin cosine distance: :
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        scale_factor: norm of input feature
        margin: margin
    :return： (theta) - m
    """

    def __init__(self, in_features, out_features, scale_factor=30.0, margin=0.40):
        super(CosineMarginHead, self).__init__()
        self.scale_factor = scale_factor
        self.margin = margin
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature, label=None):
        # --------------------------- cos(theta) & phi(theta) ---------------------------
        cosine = F.linear(F.normalize(feature), F.normalize(self.weight))

        # when test, no label, just return
        if label is None:
            return cosine * self.scale_factor

        phi = cosine - self.margin
        output = torch.where(
            one_hot(label, cosine.shape[1]).byte(), phi, cosine)
        output *= self.scale_factor

        return output


class SoftmaxMarginHead(nn.Module):
    """Implement of softmax with margin:
    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        scale_factor: norm of input feature
        margin: margin
    """

    def __init__(self, in_features, out_features, scale_factor=5.0, margin=0.40):
        super(SoftmaxMarginHead, self).__init__()
        self.scale_factor = scale_factor
        self.margin = margin
        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, feature, label=None):
        z = F.linear(feature, self.weight)
        z -= z.min(dim=1, keepdim=True)[0]
        # when test, no label, just return
        if label is None:
            return z * self.scale_factor

        phi = z - self.margin
        output = torch.where(
            one_hot(label, z.shape[1]).byte(), phi, z)
        output *= self.scale_factor

        return output
class CenterLoss(nn.Module):
    def __init__(self, num_classes=7, feat_dim=64,use_gpu=True):
        super(CenterLoss, self).__init__()
        self.num_classes = num_classes
        self.feat_dim = feat_dim
        # 初始化类别中心参数，并将其注册为模型的参数
        if use_gpu:
            self.centers = nn.Parameter(torch.randn(num_classes, feat_dim).cuda())
        else:
            self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))
    def forward(self, features, labels):
        """
        计算中心损失
        :param features: 特征张量，形状为(batch_size, feat_dim)
        :param labels: 标签张量，形状为(batch_size,)
        """
        # 根据标签获取对应的中心
        centers_batch = self.centers.index_select(dim=0, index=labels)
        # 计算特征与对应中心之间的距离
        loss = (features - centers_batch).pow(2).sum(dim=1).mean()
        return loss


class CenterLoss_cos(nn.Module):
    def __init__(self, num_classes=7, feat_dim=64, use_gpu=True):
        super(CenterLoss_cos, self).__init__()
        self.num_classes = num_classes
        self.feat_dim = feat_dim
        # 初始化类别中心参数，并将其注册为模型的参数
        if use_gpu:
            self.centers = nn.Parameter(torch.randn(num_classes, feat_dim).cuda())
        else:
            self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))
        
    def forward(self, features, labels):
        """
        计算中心损失
        :param features: 特征张量，形状为(batch_size, feat_dim)
        :param labels: 标签张量，形状为(batch_size,)
        """
        # 单位化特征向量和类别中心
        features_normalized = F.normalize(features, p=2, dim=1)
        centers_normalized = F.normalize(self.centers, p=2, dim=1)
        
        # 根据标签获取对应的中心
        centers_batch = centers_normalized.index_select(dim=0, index=labels)
        
        # 计算余弦距离（即特征向量与类别中心的点积）
        loss = 1 - torch.sum(features_normalized * centers_batch, dim=1)
        loss = torch.mean(loss)
        
        return loss
import torch
import torch.nn as nn
from torch.autograd.function import Function


class SparseCenterLoss(nn.Module):
    def __init__(self, num_classes, feat_dim, size_average=True):
        super(SparseCenterLoss, self).__init__()
        self.centers = nn.Parameter(torch.FloatTensor(num_classes, feat_dim))
        self.sparse_centerloss = SparseCenterLossFunction.apply
        self.feat_dim = feat_dim
        self.size_average = size_average
        self.reset_params()

    def reset_params(self):
        nn.init.kaiming_normal_(self.centers.data.t())

    def forward(self, feat, A, label):
        batch_size = feat.size(0)
        feat = feat.view(batch_size, -1)
        # To check the dim of centers and features
        if feat.size(1) != self.feat_dim:
            raise ValueError("Center's dim: {0} should be equal to input feature's \
                            dim: {1}".format(self.feat_dim, feat.size(1)))
        batch_size_tensor = feat.new_empty(1).fill_(batch_size if self.size_average else 1)
        loss = self.sparse_centerloss(feat, A, label, self.centers, batch_size_tensor)
        return loss


class SparseCenterLossFunction(Function):
    @staticmethod
    def forward(ctx, feature, A, label, centers, batch_size):
        ctx.save_for_backward(feature, A, label, centers, batch_size)
        centers_batch = centers.index_select(0, label.long())
        return (A * (feature - centers_batch).pow(2)).sum() / 2.0 / batch_size

    @staticmethod
    def backward(ctx, grad_output):
        feature, A, label, centers, batch_size = ctx.saved_tensors
        centers_batch = centers.index_select(0, label.long())
        diff = feature - centers_batch
        # init every iteration
        counts = centers.new_ones(centers.size(0))
        ones = centers.new_ones(label.size(0))
        grad_centers = centers.new_zeros(centers.size())

        # A gradient
        grad_A = diff.pow(2) / 2.0 / batch_size

        counts.scatter_add_(0, label.long(), ones)
        grad_centers.scatter_add_(0, label.unsqueeze(1).expand(feature.size()).long(), - A * diff)
        grad_centers = grad_centers / counts.view(-1, 1)
        return grad_output * A * diff / batch_size, grad_output * grad_A, None, grad_centers, None

SMALL_NUM = np.log(1e-45)


import torch
import torch.nn as nn

class DCL(nn.Module):  # Inherit from nn.Module
    """
    Decoupled Contrastive Loss proposed in https://arxiv.org/pdf/2110.06848.pdf
    weight: the weighting function of the positive sample loss
    temperature: temperature to control the sharpness of the distribution
    """

    def __init__(self, temperature=0.1, weight_fn=None):
        super(DCL, self).__init__()  # Initialize the superclass nn.Module
        self.temperature = temperature
        self.weight_fn = weight_fn

    def forward(self, z1, z2):  # Rename __call__ to forward for nn.Module compliance
        """
        Calculate one way DCL loss
        :param z1: first embedding vector
        :param z2: second embedding vector
        :return: one-way loss
        """
        cross_view_distance = torch.mm(z1, z2.t())
        positive_loss = -torch.diag(cross_view_distance) / self.temperature
        if self.weight_fn is not None:
            positive_loss = positive_loss * self.weight_fn(z1, z2)
        neg_similarity = torch.cat((torch.mm(z1, z1.t()), cross_view_distance), dim=1) / self.temperature
        neg_mask = torch.eye(z1.size(0), device=z1.device).repeat(1, 2)
        negative_loss = torch.logsumexp(neg_similarity + neg_mask * -1e20, dim=1, keepdim=False)  # Small number handling
        return (positive_loss + negative_loss).mean()





class DCLW(DCL):
    """
    Decoupled Contrastive Loss with negative von Mises-Fisher weighting proposed in https://arxiv.org/pdf/2110.06848.pdf
    sigma: the weighting function of the positive sample loss
    temperature: temperature to control the sharpness of the distribution
    """
    def __init__(self, sigma=0.5, temperature=0.1):
        weight_fn = lambda z1, z2: 2 - z1.size(0) * torch.nn.functional.softmax((z1 * z2).sum(dim=1) / sigma, dim=0).squeeze()
        super(DCLW, self).__init__(weight_fn=weight_fn, temperature=temperature)
